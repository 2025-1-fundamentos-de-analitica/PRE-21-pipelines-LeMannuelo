{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1208456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimator():\n",
    "    \"\"\"\n",
    "    Función principal que entrena un modelo de clasificación de texto usando regresión logística.\n",
    "    Esta función implementa un pipeline completo de procesamiento de texto y entrenamiento.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "    # ============================================================================\n",
    "    import pandas as pd  # Para manejo de datos estructurados (DataFrames)\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer  # Para transformar conteos de palabras a pesos TF-IDF\n",
    "    from sklearn.linear_model import LogisticRegression  # Algoritmo de clasificación por regresión logística\n",
    "    from sklearn.model_selection import train_test_split  # Para dividir datos en entrenamiento y prueba\n",
    "    from sklearn.pipeline import Pipeline  # Para crear un flujo de procesamiento secuencial\n",
    "    from sklearn.metrics import balanced_accuracy_score  # Métrica de evaluación balanceada\n",
    "    from sklearn.feature_extraction.text import CountVectorizer  # Para convertir texto a matriz de conteos de palabras\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score  # Métricas adicionales de evaluación\n",
    "    # import os  # Para operaciones del sistema operativo (comentado, no se usa)\n",
    "    import pickle  # Para serializar y guardar el modelo entrenado\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA Y PREPARACIÓN DE LOS DATOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el dataset desde un archivo CSV comprimido\n",
    "    # El archivo contiene frases de texto y sus etiquetas de clasificación\n",
    "    dataframe = pd.read_csv(\n",
    "        \"../files/input/sentences.csv.zip\",  # Ruta al archivo comprimido\n",
    "        index_col=False,  # No usar ninguna columna como índice\n",
    "        compression=\"zip\",  # Especificar que el archivo está comprimido en ZIP\n",
    "    )\n",
    "\n",
    "    # Separar las características (texto) de las etiquetas (target)\n",
    "    data = dataframe.phrase  # Columna con las frases de texto a clasificar\n",
    "    target = dataframe.target  # Columna con las etiquetas verdaderas (clases)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DIVISIÓN DE DATOS EN ENTRENAMIENTO Y PRUEBA\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Dividir el dataset en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data,  # Datos de entrada (frases)\n",
    "        target,  # Etiquetas objetivo\n",
    "        test_size=0.3,  # 30% para prueba, 70% para entrenamiento\n",
    "        shuffle=False,  # Mantener el orden original (importante para datos temporales)\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # CONFIGURACIÓN DEL PIPELINE DE PROCESAMIENTO DE TEXTO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Paso 1: Vectorización del texto\n",
    "    # Convierte texto crudo en una matriz numérica de conteos de palabras\n",
    "    vectorizer = CountVectorizer(\n",
    "        lowercase=True,  # Convertir todo el texto a minúsculas para normalización\n",
    "        analyzer=\"word\",  # Analizar a nivel de palabras (no caracteres)\n",
    "        token_pattern=r\"\\b[a-zA-Z]\\w+\\b\",  # Patrón regex: palabras que empiecen con letra y contengan caracteres alfanuméricos\n",
    "        stop_words=\"english\",  # Filtrar palabras comunes en inglés (the, and, is, etc.)\n",
    "    )\n",
    "\n",
    "    # Paso 2: Transformación TF-IDF\n",
    "    # Convierte los conteos brutos en pesos TF-IDF que dan más importancia a palabras discriminativas\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    # ============================================================================\n",
    "    # CREACIÓN Y CONFIGURACIÓN DEL PIPELINE COMPLETO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear un pipeline que ejecuta secuencialmente:\n",
    "    # 1. Vectorización del texto → 2. Transformación TF-IDF → 3. Clasificación\n",
    "    lr_estimator = Pipeline(\n",
    "        steps=[\n",
    "            (\"vectorizer\", vectorizer),  # Primer paso: convertir texto a matriz de conteos\n",
    "            (\"transformer\", transformer),  # Segundo paso: aplicar transformación TF-IDF\n",
    "            (\"estimator\", LogisticRegression(max_iter=1000)),  # Tercer paso: entrenar clasificador\n",
    "        ],\n",
    "        verbose=False,  # No mostrar información detallada durante el entrenamiento\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # ENTRENAMIENTO DEL MODELO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Entrenar el pipeline completo con los datos de entrenamiento\n",
    "    # Esto ejecuta automáticamente todos los pasos del pipeline en secuencia\n",
    "    lr_estimator.fit(X_train, y_train)\n",
    "\n",
    "    # ============================================================================\n",
    "    # PERSISTENCIA DEL MODELO ENTRENADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Guardar el modelo entrenado en un archivo binario para uso posterior\n",
    "    # Esto permite reutilizar el modelo sin necesidad de re-entrenarlo\n",
    "    with open(\"estimator.pickle\", \"wb\") as file:\n",
    "        pickle.dump(lr_estimator, file)  # Serializar y guardar el pipeline completo\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DE LA FUNCIÓN DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "# Llamar a la función para ejecutar todo el proceso de entrenamiento\n",
    "train_estimator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdad2b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'positive', ..., 'neutral', 'positive',\n",
       "       'neutral'], shape=(2264,), dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def use_estimator():\n",
    "    \"\"\"\n",
    "    Función que carga un modelo previamente entrenado y lo utiliza para hacer predicciones.\n",
    "    Esta función implementa la fase de inferencia del pipeline de machine learning.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍAS PARA INFERENCIA\n",
    "    # ============================================================================\n",
    "    import pickle  # Para deserializar y cargar el modelo guardado\n",
    "    import pandas as pd  # Para manejo de datos estructurados\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DE DATOS PARA PREDICCIÓN\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el mismo dataset que se usó para entrenamiento\n",
    "    # En un escenario real, estos serían datos nuevos no vistos durante el entrenamiento\n",
    "    dataframe = pd.read_csv(\n",
    "        \"../files/input/sentences.csv.zip\",  # Archivo con las frases a clasificar\n",
    "        index_col=False,  # No usar ninguna columna como índice\n",
    "        compression=\"zip\",  # El archivo está comprimido en formato ZIP\n",
    "    )\n",
    "\n",
    "    # Extraer únicamente las frases de texto (no necesitamos las etiquetas para predicción)\n",
    "    data = dataframe.phrase  # Columna con las frases que queremos clasificar\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DEL MODELO PREVIAMENTE ENTRENADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el pipeline completo desde el archivo donde fue guardado\n",
    "    # Esto incluye el vectorizador, transformador TF-IDF y el clasificador entrenado\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        estimator = pickle.load(file)  # Deserializar el modelo completo\n",
    "\n",
    "    # ============================================================================\n",
    "    # GENERACIÓN DE PREDICCIONES\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Aplicar el modelo a los nuevos datos para obtener predicciones\n",
    "    # El pipeline automáticamente:\n",
    "    # 1. Vectoriza el texto usando el mismo vocabulario del entrenamiento\n",
    "    # 2. Aplica la transformación TF-IDF con los mismos parámetros\n",
    "    # 3. Genera predicciones usando el clasificador entrenado\n",
    "    prediction = estimator.predict(data)\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DE RESULTADOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver las predicciones para que puedan ser utilizadas por otros procesos\n",
    "    return prediction\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DE LA FUNCIÓN DE PREDICCIÓN\n",
    "# ============================================================================\n",
    "\n",
    "# Llamar a la función para ejecutar el proceso de inferencia\n",
    "# El resultado serán las predicciones de clase para cada frase en el dataset\n",
    "use_estimator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
