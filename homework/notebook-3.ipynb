{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c68db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Función utilitaria para cargar y preparar los datos del dataset.\n",
    "    Esta función centraliza la carga de datos para evitar duplicación de código.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍA PARA MANEJO DE DATOS\n",
    "    # ============================================================================\n",
    "    import pandas as pd  # Librería principal para manipulación de datos estructurados\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DEL DATASET DESDE ARCHIVO COMPRIMIDO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el dataset de frases desde un archivo CSV comprimido\n",
    "    # Utiliza ruta relativa desde la ubicación del notebook\n",
    "    dataframe = pd.read_csv(\n",
    "        \"../files/input/sentences.csv.zip\",  # Ruta relativa al archivo de datos comprimido\n",
    "        index_col=False,  # No usar ninguna columna como índice del DataFrame\n",
    "        compression=\"zip\",  # Especificar que el archivo está en formato ZIP\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # SEPARACIÓN DE CARACTERÍSTICAS Y ETIQUETAS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Extraer las frases de texto que serán las características (features) del modelo\n",
    "    data = dataframe.phrase  # Columna con el texto a clasificar\n",
    "    \n",
    "    # Extraer las etiquetas objetivo que representan las clases verdaderas\n",
    "    target = dataframe.target  # Columna con las categorías/clases de cada frase\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DE DATOS SEPARADOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver por separado las características y las etiquetas\n",
    "    # Esto facilita su uso en otras funciones del pipeline\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_split(x, y):\n",
    "    \"\"\"\n",
    "    Función que divide los datos en conjuntos de entrenamiento y prueba.\n",
    "    Utiliza una semilla aleatoria para garantizar reproducibilidad de resultados.\n",
    "    \n",
    "    Args:\n",
    "        x: Características/features (frases de texto)\n",
    "        y: Etiquetas/targets (categorías de clasificación)\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE FUNCIÓN PARA DIVISIÓN DE DATOS\n",
    "    # ============================================================================\n",
    "    from sklearn.model_selection import train_test_split  # Función para dividir datasets\n",
    "\n",
    "    # ============================================================================\n",
    "    # DIVISIÓN ESTRATIFICADA DE LOS DATOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Dividir el dataset en conjuntos de entrenamiento (75%) y prueba (25%)\n",
    "    # La división es aleatoria pero reproducible gracias a random_state\n",
    "    (x_train, x_test, y_train, y_test) = train_test_split(\n",
    "        x,  # Datos de entrada (frases de texto)\n",
    "        y,  # Etiquetas correspondientes\n",
    "        test_size=0.25,  # 25% de los datos para conjunto de prueba\n",
    "        random_state=123456,  # Semilla para reproducibilidad de la división aleatoria\n",
    "    )\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RETORNO DE CONJUNTOS DIVIDIDOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver los cuatro conjuntos resultantes:\n",
    "    # - x_train, y_train: para entrenar el modelo\n",
    "    # - x_test, y_test: para evaluar el rendimiento del modelo\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(estimator):\n",
    "    \"\"\"\n",
    "    Función que construye un pipeline de procesamiento de texto completo.\n",
    "    Crea una secuencia de transformaciones que preparan el texto para clasificación.\n",
    "    \n",
    "    Args:\n",
    "        estimator: Algoritmo de machine learning a usar (ej: LogisticRegression, MLPClassifier)\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE COMPONENTES DEL PIPELINE\n",
    "    # ============================================================================\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # Para procesamiento de texto\n",
    "    from sklearn.pipeline import Pipeline  # Para crear secuencias de transformaciones\n",
    "\n",
    "    # ============================================================================\n",
    "    # CONFIGURACIÓN DEL VECTORIZADOR DE TEXTO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Primer componente: Convierte texto crudo en matriz numérica de conteos\n",
    "    vectorizer = CountVectorizer(\n",
    "        lowercase=True,  # Normalizar texto convirtiendo a minúsculas\n",
    "        analyzer=\"word\",  # Procesar a nivel de palabras individuales\n",
    "        token_pattern=r\"\\b[a-zA-Z]\\w+\\b\",  # Regex: palabras que empiecen con letra y contengan alfanuméricos\n",
    "        stop_words=\"english\",  # Filtrar palabras comunes del inglés (the, and, is, etc.)\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # CONFIGURACIÓN DEL TRANSFORMADOR TF-IDF\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Segundo componente: Convierte conteos brutos en pesos TF-IDF\n",
    "    # TF-IDF da mayor peso a palabras discriminativas y menor a palabras comunes\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    # ============================================================================\n",
    "    # CONSTRUCCIÓN DEL PIPELINE SECUENCIAL\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear pipeline que ejecuta transformaciones en orden secuencial:\n",
    "    # texto → conteos → TF-IDF → clasificación\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"vectorizer\", vectorizer),  # Paso 1: Convertir texto a matriz de conteos\n",
    "            (\"transformer\", transformer),  # Paso 2: Aplicar transformación TF-IDF\n",
    "            (\"estimator\", estimator),  # Paso 3: Aplicar algoritmo de clasificación\n",
    "        ],\n",
    "        verbose=False,  # No mostrar información detallada durante ejecución\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DEL PIPELINE CONFIGURADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver el pipeline completo listo para entrenamiento\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d90508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid_search(estimator, param_grid, cv=5):\n",
    "    \"\"\"\n",
    "    Función que configura búsqueda exhaustiva de hiperparámetros.\n",
    "    Utiliza validación cruzada para encontrar la mejor combinación de parámetros.\n",
    "    \n",
    "    Args:\n",
    "        estimator: Pipeline o modelo a optimizar\n",
    "        param_grid: Diccionario con parámetros y valores a probar\n",
    "        cv: Número de folds para validación cruzada (default=5)\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE HERRAMIENTA PARA BÚSQUEDA DE HIPERPARÁMETROS\n",
    "    # ============================================================================\n",
    "    from sklearn.model_selection import GridSearchCV  # Para búsqueda exhaustiva de parámetros\n",
    "\n",
    "    # ============================================================================\n",
    "    # CONFIGURACIÓN DE LA BÚSQUEDA GRID SEARCH\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear objeto GridSearchCV que probará todas las combinaciones de parámetros\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,  # Modelo/pipeline a optimizar\n",
    "        param_grid=param_grid,  # Diccionario con parámetros a probar\n",
    "        cv=cv,  # Número de folds para validación cruzada (5 por defecto)\n",
    "        scoring=\"balanced_accuracy\",  # Métrica para evaluar combinaciones (balanceada para clases desbalanceadas)\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DEL OBJETO GRID SEARCH CONFIGURADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver el objeto GridSearchCV listo para entrenar\n",
    "    # Al hacer fit(), probará todas las combinaciones y seleccionará la mejor\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_estimator(estimator):\n",
    "    \"\"\"\n",
    "    Función que guarda un modelo entrenado en disco para uso posterior.\n",
    "    Utiliza serialización binaria para preservar el estado completo del modelo.\n",
    "    \n",
    "    Args:\n",
    "        estimator: Modelo/pipeline entrenado que se desea persistir\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍA PARA SERIALIZACIÓN\n",
    "    # ============================================================================\n",
    "    import pickle  # Librería estándar de Python para serialización de objetos\n",
    "\n",
    "    # ============================================================================\n",
    "    # PERSISTENCIA DEL MODELO EN DISCO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Abrir archivo en modo escritura binaria para guardar el modelo\n",
    "    # El contexto 'with' garantiza que el archivo se cierre correctamente\n",
    "    with open(\"estimator.pickle\", \"wb\") as file:\n",
    "        # Serializar y escribir el modelo completo al archivo\n",
    "        # Esto incluye el pipeline, parámetros entrenados y configuraciones\n",
    "        pickle.dump(estimator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator():\n",
    "    \"\"\"\n",
    "    Función que carga un modelo previamente guardado desde disco.\n",
    "    Incluye verificación de existencia del archivo para evitar errores.\n",
    "    \n",
    "    Returns:\n",
    "        estimator: Modelo cargado desde disco, o None si no existe\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "    # ============================================================================\n",
    "    import os  # Para verificar existencia de archivos\n",
    "    import pickle  # Para deserializar objetos guardados\n",
    "\n",
    "    # ============================================================================\n",
    "    # VERIFICACIÓN DE EXISTENCIA DEL ARCHIVO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Comprobar si existe un modelo previamente guardado\n",
    "    # Esto evita errores cuando no hay un modelo previo\n",
    "    if not os.path.exists(\"estimator.pickle\"):\n",
    "        return None  # Retornar None si no hay modelo guardado\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DEL MODELO DESDE DISCO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Abrir archivo en modo lectura binaria para cargar el modelo\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        # Deserializar y cargar el modelo completo desde el archivo\n",
    "        estimator = pickle.load(file)\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DEL MODELO CARGADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver el modelo cargado (incluyendo pipeline y parámetros entrenados)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cd65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression():\n",
    "    \"\"\"\n",
    "    Función principal para entrenar regresión logística con optimización de hiperparámetros.\n",
    "    Implementa búsqueda exhaustiva de parámetros y comparación con modelos previos.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE ALGORITMO Y MÉTRICAS\n",
    "    # ============================================================================\n",
    "    from sklearn.linear_model import LogisticRegression  # Algoritmo de regresión logística\n",
    "    from sklearn.metrics import balanced_accuracy_score  # Métrica balanceada para evaluación\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 1: CARGA DE DATOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el dataset completo usando la función utilitaria\n",
    "    data, target = load_data()\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 2: DIVISIÓN DE DATOS EN ENTRENAMIENTO Y PRUEBA\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Dividir los datos manteniendo reproducibilidad\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(\n",
    "        x=data,  # Frases de texto como características\n",
    "        y=target,  # Categorías como etiquetas objetivo\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 3: CONSTRUCCIÓN DEL PIPELINE BASE\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear pipeline completo con regresión logística\n",
    "    # max_iter=1000: suficientes iteraciones para convergencia\n",
    "    pipeline = make_pipeline(\n",
    "        estimator=LogisticRegression(max_iter=1000),\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 4: CONFIGURACIÓN DE BÚSQUEDA DE HIPERPARÁMETROS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear GridSearchCV para optimizar parámetros del TF-IDF transformer\n",
    "    estimator = make_grid_search(\n",
    "        estimator=pipeline,\n",
    "        param_grid={\n",
    "            # Parámetros del transformador TF-IDF a optimizar:\n",
    "            \"transformer__norm\": [\"l1\", \"l2\", None],  # Normalización: L1, L2 o sin normalizar\n",
    "            \"transformer__use_idf\": [True, False],  # Usar o no el componente IDF\n",
    "            \"transformer__smooth_idf\": [True, False],  # Suavizado del IDF para evitar división por cero\n",
    "        },\n",
    "        cv=5,  # Validación cruzada con 5 folds\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 5: ENTRENAMIENTO CON BÚSQUEDA DE HIPERPARÁMETROS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Entrenar el modelo probando todas las combinaciones de parámetros\n",
    "    # GridSearchCV automáticamente selecciona la mejor combinación\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 6: COMPARACIÓN CON MODELO PREVIAMENTE GUARDADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Intentar cargar un modelo previamente entrenado para comparación\n",
    "    best_estimator = load_estimator()\n",
    "\n",
    "    # Si existe un modelo previo, comparar rendimientos\n",
    "    if best_estimator is not None:\n",
    "\n",
    "        # Evaluar el modelo previamente guardado en el conjunto de prueba\n",
    "        saved_balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true=y_test, y_pred=best_estimator.predict(x_test)\n",
    "        )\n",
    "\n",
    "        # Evaluar el modelo recién entrenado en el conjunto de prueba\n",
    "        current_balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true=y_test, y_pred=estimator.predict(x_test)\n",
    "        )\n",
    "\n",
    "        # Si el modelo anterior es mejor, mantenerlo\n",
    "        # Solo actualizar si el nuevo modelo supera al anterior\n",
    "        if current_balanced_accuracy < saved_balanced_accuracy:\n",
    "            estimator = best_estimator\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 7: PERSISTENCIA DEL MEJOR MODELO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Guardar el mejor modelo (nuevo o previo) para uso futuro\n",
    "    save_estimator(estimator)\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DEL ENTRENAMIENTO DE REGRESIÓN LOGÍSTICA\n",
    "# ============================================================================\n",
    "\n",
    "# Ejecutar el proceso completo de entrenamiento con optimización\n",
    "train_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359f335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'positive', ..., 'negative', 'negative',\n",
       "       'negative'], shape=(2264,), dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def use_estimator():\n",
    "    \"\"\"\n",
    "    Función que utiliza el mejor modelo entrenado para generar predicciones.\n",
    "    Implementa la fase de inferencia del proceso de machine learning.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "    # ============================================================================\n",
    "    import pickle  # Para cargar el modelo serializado desde disco\n",
    "    import pandas as pd  # Para manejo de datos estructurados\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DE DATOS PARA REALIZAR PREDICCIONES\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el dataset de frases que queremos clasificar\n",
    "    # En un escenario real, estos serían datos nuevos no vistos durante entrenamiento\n",
    "    dataframe = pd.read_csv(\n",
    "        \"../files/input/sentences.csv.zip\",  # Ruta relativa al archivo de datos\n",
    "        index_col=False,  # No usar ninguna columna como índice\n",
    "        compression=\"zip\",  # El archivo está comprimido en formato ZIP\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # EXTRACCIÓN DE CARACTERÍSTICAS PARA PREDICCIÓN\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Extraer únicamente las frases de texto (no necesitamos las etiquetas)\n",
    "    # Estas serán las características de entrada para el modelo\n",
    "    data = dataframe.phrase\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DEL MEJOR MODELO DESDE DISCO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el mejor modelo que fue guardado (puede ser de cualquier algoritmo)\n",
    "    # Esto incluye el pipeline completo con los mejores hiperparámetros\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        estimator = pickle.load(file)  # Deserializar el modelo optimizado\n",
    "\n",
    "    # ============================================================================\n",
    "    # GENERACIÓN DE PREDICCIONES\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Aplicar el modelo optimizado a los datos para obtener predicciones\n",
    "    # El pipeline automáticamente ejecuta toda la secuencia optimizada:\n",
    "    # 1. Vectorización del texto con el vocabulario aprendido\n",
    "    # 2. Transformación TF-IDF con los mejores parámetros encontrados\n",
    "    # 3. Clasificación usando el mejor modelo entrenado\n",
    "    prediction = estimator.predict(data)\n",
    "\n",
    "    # ============================================================================\n",
    "    # RETORNO DE PREDICCIONES\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Devolver las predicciones de clase para cada frase\n",
    "    # Resultado: array con la clase predicha para cada entrada\n",
    "    return prediction\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DEL PROCESO DE INFERENCIA\n",
    "# ============================================================================\n",
    "\n",
    "# Ejecutar la función para generar predicciones usando el mejor modelo\n",
    "use_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47658072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vectorizer',\n",
      "                 CountVectorizer(stop_words='english',\n",
      "                                 token_pattern='\\\\b[a-zA-Z]\\\\w+\\\\b')),\n",
      "                ('transformer', TfidfTransformer(norm=None, smooth_idf=False)),\n",
      "                ('estimator', LogisticRegression(max_iter=1000))]):\n",
      "  Balanced Accuracy: 0.7165 (0.9962)\n",
      "           Accuracy: 0.8233 (0.9982)\n"
     ]
    }
   ],
   "source": [
    "def check_estimator():\n",
    "    \"\"\"\n",
    "    Función de diagnóstico que evalúa el rendimiento del mejor modelo guardado.\n",
    "    Calcula métricas en entrenamiento y prueba para detectar overfitting/underfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "    # ============================================================================\n",
    "    import pickle  # Para cargar el modelo guardado\n",
    "    import pandas as pd  # Para manejo de datos\n",
    "    from sklearn.metrics import accuracy_score, balanced_accuracy_score  # Métricas de evaluación\n",
    "\n",
    "    # ============================================================================\n",
    "    # PREPARACIÓN DE DATOS PARA EVALUACIÓN\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar los mismos datos usados durante el entrenamiento\n",
    "    data, target = load_data()\n",
    "\n",
    "    # Dividir exactamente igual que durante el entrenamiento para comparación válida\n",
    "    x_train, x_test, y_train_true, y_test_true = make_train_test_split(\n",
    "        x=data,  # Mismas características\n",
    "        y=target,  # Mismas etiquetas\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # CARGA DEL MEJOR MODELO GUARDADO\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el modelo optimizado desde disco\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        estimator = pickle.load(file)  # Modelo con mejores hiperparámetros\n",
    "\n",
    "    # ============================================================================\n",
    "    # GENERACIÓN DE PREDICCIONES EN AMBOS CONJUNTOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Predicciones en el conjunto de entrenamiento\n",
    "    # Esto nos permite evaluar qué tan bien aprendió de los datos de entrenamiento\n",
    "    y_train_pred = estimator.predict(x_train)\n",
    "    \n",
    "    # Predicciones en el conjunto de prueba\n",
    "    # Esto nos permite evaluar la capacidad de generalización del modelo\n",
    "    y_test_pred = estimator.predict(x_test)\n",
    "\n",
    "    # ============================================================================\n",
    "    # CÁLCULO DE MÉTRICAS DE EVALUACIÓN\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Métricas en conjunto de entrenamiento\n",
    "    accuracy_train = round(accuracy_score(y_train_true, y_train_pred), 4)\n",
    "    balanced_accuracy_train = round(\n",
    "        balanced_accuracy_score(y_train_true, y_train_pred), 4\n",
    "    )\n",
    "    \n",
    "    # Métricas en conjunto de prueba\n",
    "    accuracy_test = round(accuracy_score(y_test_true, y_test_pred), 4)\n",
    "    balanced_accuracy_test = round(balanced_accuracy_score(y_test_true, y_test_pred), 4)\n",
    "\n",
    "    # ============================================================================\n",
    "    # REPORTE DE RESULTADOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Mostrar el mejor modelo encontrado con sus hiperparámetros optimizados\n",
    "    print(estimator.best_estimator_, \":\", sep=\"\")\n",
    "    \n",
    "    # Mostrar métricas: test (train) - formato que permite comparar fácilmente\n",
    "    # Si train >> test: posible overfitting\n",
    "    # Si train ≈ test: buen balance bias-varianza\n",
    "    print(f\"  Balanced Accuracy: {balanced_accuracy_test} ({balanced_accuracy_train})\")\n",
    "    print(f\"           Accuracy: {accuracy_test} ({accuracy_train})\")\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DE LA EVALUACIÓN DEL MODELO\n",
    "# ============================================================================\n",
    "\n",
    "# Ejecutar la evaluación completa del mejor modelo guardado\n",
    "check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502cc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Samuel Castaño\\Desktop\\Fundamentos Analtica 2025-1\\Clases\\Clase 10\\PRE-21-pipelines-SamuCasta\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:787: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "def train_mlp_classifier():\n",
    "    \"\"\"\n",
    "    Función para entrenar un clasificador de redes neuronales (MLP) con optimización de hiperparámetros.\n",
    "    Implementa búsqueda exhaustiva y comparación con el mejor modelo existente.\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # IMPORTACIÓN DE ALGORITMO Y MÉTRICAS\n",
    "    # ============================================================================\n",
    "    from sklearn.metrics import balanced_accuracy_score  # Métrica balanceada para evaluación\n",
    "    from sklearn.neural_network import MLPClassifier  # Clasificador de redes neuronales multicapa\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 1: CARGA DE DATOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el dataset completo usando la función utilitaria\n",
    "    data, target = load_data()\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 2: DIVISIÓN DE DATOS EN ENTRENAMIENTO Y PRUEBA\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Dividir los datos manteniendo reproducibilidad (misma semilla que otros modelos)\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(\n",
    "        x=data,  # Frases de texto como características\n",
    "        y=target,  # Categorías como etiquetas objetivo\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 3: CONSTRUCCIÓN DEL PIPELINE CON RED NEURONAL\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear pipeline completo con clasificador MLP\n",
    "    # max_iter=10000: suficientes iteraciones para convergencia de la red neuronal\n",
    "    pipeline = make_pipeline(\n",
    "        estimator=MLPClassifier(max_iter=10000),\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 4: CONFIGURACIÓN EXTENSA DE BÚSQUEDA DE HIPERPARÁMETROS\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Crear GridSearchCV para optimizar múltiples aspectos del modelo\n",
    "    estimator = make_grid_search(\n",
    "        estimator=pipeline,\n",
    "        param_grid={\n",
    "            # Parámetros del transformador TF-IDF:\n",
    "            \"transformer__norm\": [\"l1\", \"l2\", None],  # Normalización del TF-IDF\n",
    "            \"transformer__use_idf\": [True, False],  # Activar/desactivar componente IDF\n",
    "            \"transformer__smooth_idf\": [True, False],  # Suavizado para evitar log(0)\n",
    "            \n",
    "            # Parámetros específicos de la red neuronal:\n",
    "            \"estimator__hidden_layer_sizes\": [(1,), (5,), (5, 5)],  # Arquitecturas: 1 neurona, 5 neuronas, 2 capas de 5\n",
    "            \"estimator__solver\": [\"adam\"],  # Optimizador Adam (eficiente para problemas medianos)\n",
    "            \"estimator__learning_rate_init\": [0.01, 0.001, 0.0001],  # Tasa de aprendizaje inicial\n",
    "        },\n",
    "        cv=5,  # Validación cruzada con 5 folds\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 5: ENTRENAMIENTO CON BÚSQUEDA EXHAUSTIVA\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Entrenar la red neuronal probando todas las combinaciones posibles\n",
    "    # Esto puede tomar considerablemente más tiempo que regresión logística\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 6: COMPARACIÓN CON MEJOR MODELO EXISTENTE\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Cargar el mejor modelo previamente guardado (puede ser LogisticRegression u otro MLP)\n",
    "    best_estimator = load_estimator()\n",
    "\n",
    "    # Si existe un modelo previo, realizar competencia entre modelos\n",
    "    if best_estimator is not None:\n",
    "\n",
    "        # Evaluar el modelo anteriormente guardado\n",
    "        saved_balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true=y_test, y_pred=best_estimator.predict(x_test)\n",
    "        )\n",
    "\n",
    "        # Evaluar el nuevo modelo MLP entrenado\n",
    "        current_balanced_accuracy = balanced_accuracy_score(\n",
    "            y_true=y_test, y_pred=estimator.predict(x_test)\n",
    "        )\n",
    "\n",
    "        # Conservar el modelo anterior si sigue siendo superior\n",
    "        # Solo reemplazar si el nuevo MLP supera al modelo existente\n",
    "        if current_balanced_accuracy < saved_balanced_accuracy:\n",
    "            estimator = best_estimator\n",
    "\n",
    "    # ============================================================================\n",
    "    # PASO 7: PERSISTENCIA DEL MEJOR MODELO GLOBAL\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Guardar el mejor modelo (nuevo MLP o modelo previo) para uso futuro\n",
    "    save_estimator(estimator)\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUCIÓN DEL ENTRENAMIENTO DE RED NEURONAL\n",
    "# ============================================================================\n",
    "\n",
    "# Ejecutar el proceso completo de entrenamiento y optimización del MLP\n",
    "train_mlp_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c21c89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcheck_estimator\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'check_estimator' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUACIÓN FINAL DEL MEJOR MODELO DESPUÉS DE ENTRENAR MLP\n",
    "# ============================================================================\n",
    "\n",
    "# Ejecutar diagnóstico final para ver si el MLP mejoró el rendimiento general\n",
    "# Esto mostrará cuál algoritmo (LogisticRegression vs MLPClassifier) resultó mejor\n",
    "# y sus métricas finales de rendimiento\n",
    "check_estimator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
